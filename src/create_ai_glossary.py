#!/usr/bin/env python3
"""
AI æœ¯è¯­è¯å…¸ç”Ÿæˆå·¥å…·
åŸºäºæœºå™¨ä¹‹å¿ƒäººå·¥æ™ºèƒ½æœ¯è¯­æ•°æ®åº“åˆ›å»ºä¸“ä¸šçš„ç¿»è¯‘è¯å…¸

GitHub: https://github.com/jiqizhixin/Artificial-Intelligence-Terminology-Database
"""

import json
import requests
import csv
import os
from io import StringIO

def download_terminology_data():
    """
    ä»æœºå™¨ä¹‹å¿ƒ GitHub ä»“åº“ä¸‹è½½æœ¯è¯­æ•°æ®
    """
    print("æ­£åœ¨ä¸‹è½½æœºå™¨ä¹‹å¿ƒäººå·¥æ™ºèƒ½æœ¯è¯­æ•°æ®åº“...")
    
    # ä¸»è¦æœ¯è¯­æ–‡ä»¶çš„ GitHub raw URL
    base_urls = {
        "é€šç”¨æœ¯è¯­": "https://raw.githubusercontent.com/jiqizhixin/Artificial-Intelligence-Terminology-Database/master/æ•°æ®æ–‡ä»¶/AIæœ¯è¯­åº“ï¼ˆæœºå™¨ä¹‹å¿ƒç‰ˆï¼‰.csv",
        "æœºå™¨å­¦ä¹ ": "https://raw.githubusercontent.com/jiqizhixin/Artificial-Intelligence-Terminology-Database/master/æ•°æ®æ–‡ä»¶/æœºå™¨å­¦ä¹ æœ¯è¯­åº“.csv",
        "AI_for_Science": "https://raw.githubusercontent.com/jiqizhixin/Artificial-Intelligence-Terminology-Database/master/æ•°æ®æ–‡ä»¶/AI for Scienceæœ¯è¯­åº“.csv"
    }
    
    all_terms = {}
    
    for category, url in base_urls.items():
        try:
            print(f"ä¸‹è½½ {category} æœ¯è¯­...")
            response = requests.get(url, timeout=30)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                # è§£æ CSV æ•°æ®
                csv_content = StringIO(response.text)
                csv_reader = csv.DictReader(csv_content)
                
                category_count = 0
                for row in csv_reader:
                    # è·å–è‹±æ–‡æœ¯è¯­å’Œä¸­æ–‡ç¿»è¯‘
                    english_term = row.get('è‹±æ–‡', '').strip()
                    chinese_term = row.get('ä¸­æ–‡', '').strip()
                    
                    # å¤„ç†å¯èƒ½çš„åˆ«åå­—æ®µ
                    abbr = row.get('ç¼©å†™', '').strip()
                    
                    if english_term and chinese_term:
                        # ä¸»æœ¯è¯­
                        all_terms[english_term] = chinese_term
                        category_count += 1
                        
                        # å¦‚æœæœ‰ç¼©å†™ï¼Œä¹ŸåŠ å…¥è¯å…¸
                        if abbr and abbr != english_term:
                            all_terms[abbr] = chinese_term
                            category_count += 1
                
                print(f"  æˆåŠŸåŠ è½½ {category_count} ä¸ªæœ¯è¯­")
            else:
                print(f"  è­¦å‘Šï¼šæ— æ³•ä¸‹è½½ {category} æœ¯è¯­ (HTTP {response.status_code})")
                
        except Exception as e:
            print(f"  é”™è¯¯ï¼šä¸‹è½½ {category} æ—¶å‡ºç°é—®é¢˜: {e}")
            continue
    
    return all_terms

def create_comprehensive_glossary():
    """
    åˆ›å»ºç»¼åˆçš„ AI æœ¯è¯­è¯å…¸æ–‡ä»¶
    """
    print("=== AI æœ¯è¯­è¯å…¸ç”Ÿæˆå·¥å…· ===")
    print("åŸºäºæœºå™¨ä¹‹å¿ƒäººå·¥æ™ºèƒ½æœ¯è¯­æ•°æ®åº“")
    print("GitHub: https://github.com/jiqizhixin/Artificial-Intelligence-Terminology-Database")
    print()
    
    # ä¸‹è½½æœ¯è¯­æ•°æ®
    ai_terms = download_terminology_data()
    
    if not ai_terms:
        print("é”™è¯¯ï¼šæ— æ³•è·å–æœ¯è¯­æ•°æ®ï¼Œå°†åˆ›å»ºåŸºç¡€ç¤ºä¾‹è¯å…¸")
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€çš„ AI æœ¯è¯­è¯å…¸ä½œä¸ºåå¤‡
        ai_terms = {
            "Artificial Intelligence": "äººå·¥æ™ºèƒ½",
            "Machine Learning": "æœºå™¨å­¦ä¹ ", 
            "Deep Learning": "æ·±åº¦å­¦ä¹ ",
            "Neural Network": "ç¥ç»ç½‘ç»œ",
            "Natural Language Processing": "è‡ªç„¶è¯­è¨€å¤„ç†",
            "Computer Vision": "è®¡ç®—æœºè§†è§‰",
            "Reinforcement Learning": "å¼ºåŒ–å­¦ä¹ ",
            "Supervised Learning": "ç›‘ç£å­¦ä¹ ",
            "Unsupervised Learning": "æ— ç›‘ç£å­¦ä¹ ",
            "Feature Engineering": "ç‰¹å¾å·¥ç¨‹",
            "Gradient Descent": "æ¢¯åº¦ä¸‹é™",
            "Backpropagation": "åå‘ä¼ æ’­",
            "Convolutional Neural Network": "å·ç§¯ç¥ç»ç½‘ç»œ",
            "Recurrent Neural Network": "å¾ªç¯ç¥ç»ç½‘ç»œ",
            "Transformer": "å˜æ¢å™¨",
            "Attention Mechanism": "æ³¨æ„åŠ›æœºåˆ¶",
            "Generative Adversarial Network": "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ",
            "Large Language Model": "å¤§è¯­è¨€æ¨¡å‹",
            "Fine-tuning": "å¾®è°ƒ",
            "Pre-training": "é¢„è®­ç»ƒ"
        }
    
    # åˆå¹¶ç°æœ‰çš„ç¤ºä¾‹è¯å…¸ä¸­æœ‰ç”¨çš„æœ¯è¯­
    additional_terms = {
        "API": "åº”ç”¨ç¨‹åºæ¥å£",
        "JavaScript": "JavaScript",
        "Python": "Python", 
        "New York": "çº½çº¦",
        "Los Angeles": "æ´›æ‰çŸ¶",
        "Silicon Valley": "ç¡…è°·",
        "Wall Street": "åå°”è¡—",
        "United States": "ç¾å›½",
        "China": "ä¸­å›½",
        "Europe": "æ¬§æ´²"
    }
    
    # åˆå¹¶æ‰€æœ‰æœ¯è¯­
    comprehensive_glossary = {**ai_terms, **additional_terms}
    
    # ä¿å­˜ä¸º JSON æ–‡ä»¶
    output_file = "ai_terminology_glossary.json"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_glossary, f, ensure_ascii=False, indent=2, sort_keys=True)
        
        print(f"âœ… æˆåŠŸåˆ›å»º AI æœ¯è¯­è¯å…¸ï¼š{output_file}")
        print(f"ğŸ“Š æ€»è®¡åŒ…å« {len(comprehensive_glossary)} ä¸ªæœ¯è¯­")
        print()
        print("ä½¿ç”¨æ–¹æ³•ï¼š")
        print(f"python translate_srt_batch.py your_file.srt -g {output_file}")
        print()
        print("æœ¯è¯­è¯å…¸å·²æŒ‰å­—æ¯é¡ºåºæ’åºï¼ŒåŒ…å«ä»¥ä¸‹ç±»åˆ«ï¼š")
        print("- æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ æœ¯è¯­")
        print("- AI for Science ä¸“ä¸šæœ¯è¯­") 
        print("- é€šç”¨äººå·¥æ™ºèƒ½æœ¯è¯­")
        print("- ç¼–ç¨‹ä¸æŠ€æœ¯æœ¯è¯­")
        print("- åœ°åä¸å¸¸ç”¨è¯æ±‡")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ ä¿å­˜è¯å…¸æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}")
        return None

def preview_glossary(file_path, num_terms=10):
    """
    é¢„è§ˆè¯å…¸å†…å®¹
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            glossary = json.load(f)
        
        print(f"\nğŸ“– è¯å…¸é¢„è§ˆï¼ˆæ˜¾ç¤ºå‰ {num_terms} ä¸ªæœ¯è¯­ï¼‰ï¼š")
        print("-" * 50)
        
        for i, (en_term, cn_term) in enumerate(list(glossary.items())[:num_terms]):
            print(f"{i+1:2d}. {en_term:<30} â†’ {cn_term}")
        
        if len(glossary) > num_terms:
            print(f"    ... è¿˜æœ‰ {len(glossary) - num_terms} ä¸ªæœ¯è¯­")
            
    except Exception as e:
        print(f"é¢„è§ˆè¯å…¸æ—¶å‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    # åˆ›å»ºç»¼åˆè¯å…¸
    glossary_file = create_comprehensive_glossary()
    
    if glossary_file:
        # é¢„è§ˆè¯å…¸å†…å®¹
        preview_glossary(glossary_file, 15)
        
        print(f"\nğŸ¯ è¯å…¸æ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š{glossary_file}")
        print("æ‚¨ç°åœ¨å¯ä»¥åœ¨å­—å¹•ç¿»è¯‘ä¸­ä½¿ç”¨è¿™ä¸ªä¸“ä¸šçš„ AI æœ¯è¯­è¯å…¸äº†ï¼") 